{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 -- Content Analysis\n",
    "\n",
    "This notebook contains the code for the qualitative analyses in section 4 (e.g., vocabulary analyses, top ngrams etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_experiments = [\n",
    "    \"base_prompt_v2_temperature_0.7\",\n",
    "    \"fixed_examples_post_v2\",\n",
    "    \"random_examples_post_v2\",\n",
    "    \"imitation_random_examples_ht_v2_temperature_0.7\",\n",
    "    \"Real\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instasynth import evaluation\n",
    "\n",
    "ta_real = evaluation.TextAnalyser(\n",
    "    data=pd.read_pickle(\"../data/df_sample.pkl\"), remove_stopwords=False\n",
    ")\n",
    "_ = ta_real._ngram_metrics()\n",
    "real_vocab = {n: set([k[0] for k in ta_real._ngrams[n]]) for n in [1, 2, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "\n",
    "def ngram_overlap(experiment: str):\n",
    "    ta = evaluation.TextAnalyser(\n",
    "        data=pd.read_pickle(f\"../results/{experiment}/final_df.pkl\"),\n",
    "        remove_stopwords=False,\n",
    "    )\n",
    "    _ = (ta._ngram_metrics(),)\n",
    "    syn_vocab = {n: set([k[0] for k in ta._ngrams[n]]) for n in [1, 2, 3]}\n",
    "    return {\n",
    "        f\"{n}gram_overlap\": len(syn_vocab[n].intersection(real_vocab[n]))\n",
    "        / len(syn_vocab[n])\n",
    "        * 100\n",
    "        for n in [1, 2, 3]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_ngram_count(df: pd.DataFrame):\n",
    "    df = df.query(\"caption != ''\")\n",
    "    df[\"caption\"] = df.caption.str.lower()\n",
    "    for n in [1, 2, 3]:\n",
    "        df[f\"{n}_gram\"] = df.caption.apply(\n",
    "            lambda x: ngrams(\n",
    "                [w for w in tt.tokenize(x) if w not in sw and len(w) > 1], n\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        n: Counter([k for l in df[f\"{n}_gram\"].tolist() for k in l]) for n in [1, 2, 3]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counters = {\n",
    "    f: get_ngram_count(pd.read_pickle(f\"../results/{f}/final_df.pkl\"))\n",
    "    for f in selected_experiments\n",
    "    if f != \"Real\"\n",
    "}\n",
    "ngram_counters[\"Real\"] = get_ngram_count(pd.read_pickle(\"../data/df_sample.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(\"../data/full_df_posts.pkl\").query(\n",
    "    \"caption != '' and country == 'US'\"\n",
    ")\n",
    "full_data[\"sponsorship\"] = full_data.has_disclosures.apply(\n",
    "    lambda x: \"sponsored\" if x else \"nonsponsored\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_real(full_df: pd.DataFrame, seed: int):\n",
    "    spons = full_df.query(\"sponsorship == 'sponsored'\").sample(500, random_state=seed)\n",
    "    nonspons = full_df.query(\"sponsorship == 'nonsponsored'\").sample(\n",
    "        500, random_state=seed\n",
    "    )\n",
    "    return pd.concat([spons, nonspons]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_tag_count(exp: str, ngram_counters: dict, tag: str = \"#\"):\n",
    "    return {\n",
    "        k[0][0]: k[1]\n",
    "        for k in ngram_counters[exp][1].most_common(100000)\n",
    "        if k[0][0].startswith(tag)\n",
    "    }\n",
    "\n",
    "\n",
    "hashtag_counter = {\n",
    "    f: get_tag_count(f, ngram_counters, \"#\")\n",
    "    for f in selected_experiments\n",
    "    if f != \"Real\"\n",
    "}\n",
    "usertag_counter = {\n",
    "    f: get_tag_count(f, ngram_counters, \"@\")\n",
    "    for f in selected_experiments\n",
    "    if f != \"Real\"\n",
    "}\n",
    "\n",
    "bootstrap_tag_counter = defaultdict(dict)\n",
    "\n",
    "for i in range(100):\n",
    "    df = _sample_real(full_df=full_data, seed=i)\n",
    "    for tag, c_tag in {\"#\": \"hashtags\", \"@\": \"usertags\"}.items():\n",
    "        df[c_tag] = df.caption.str.lower().apply(\n",
    "            lambda x: ngrams(\n",
    "                [w for w in tt.tokenize(x) if w.startswith(tag) and len(w) > 1], 1\n",
    "            )\n",
    "        )\n",
    "        tag_counter = Counter()\n",
    "        df[c_tag].apply(tag_counter.update)\n",
    "        for k, v in {k[0][0]: k[1] for k in tag_counter.most_common(100000)}.items():\n",
    "            if k not in bootstrap_tag_counter[c_tag]:\n",
    "                bootstrap_tag_counter[c_tag][k] = 0\n",
    "            bootstrap_tag_counter[c_tag][k] += v\n",
    "\n",
    "bootstrap_tag_counter = {\n",
    "    k: {k2: v2 / 100 for k2, v2 in v.items()} for k, v in bootstrap_tag_counter.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = bootstrap_tag_counter[\"hashtags\"]\n",
    "usertags = bootstrap_tag_counter[\"usertags\"]\n",
    "sorted_hashtags = {\n",
    "    k: hashtags[k] for k in sorted(hashtags, key=hashtags.get, reverse=True)\n",
    "}\n",
    "sorted_usertags = {\n",
    "    k: usertags[k] for k in sorted(usertags, key=usertags.get, reverse=True)\n",
    "}\n",
    "\n",
    "hashtag_counter[\"Real\"] = sorted_hashtags\n",
    "usertag_counter[\"Real\"] = sorted_usertags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counter = defaultdict(dict)\n",
    "\n",
    "for exp, ngs in ngram_counters.items():\n",
    "    for n, counts in ngs.items():\n",
    "        ngram_counter[exp][n] = {\n",
    "            \" \".join(k[0]): k[1]\n",
    "            for k in counts.most_common(100000)\n",
    "            if not k[0][0].startswith(\"#\") and not k[0][0].startswith(\"@\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_entities(count_dict, topn=100):\n",
    "    return set([k for k in count_dict.keys()][:topn])\n",
    "\n",
    "\n",
    "def get_overlap_top_n_entities(count_dict1, count_dict2, topn=100):\n",
    "    top_tokens_1 = get_top_n_entities(count_dict1, topn)\n",
    "    top_tokens_2 = get_top_n_entities(count_dict2, topn)\n",
    "    return len(top_tokens_1.intersection(top_tokens_2)) / topn * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_overlap = defaultdict(dict)\n",
    "\n",
    "for exp in _selected_experiments:\n",
    "    top100_overlap[exp][\"hashtags\"] = get_overlap_top_n_entities(\n",
    "        hashtag_counter[exp], hashtag_counter[\"Real\"], topn=100\n",
    "    )\n",
    "    top100_overlap[exp][\"usertags\"] = get_overlap_top_n_entities(\n",
    "        usertag_counter[exp], usertag_counter[\"Real\"], topn=100\n",
    "    )\n",
    "    for n in [1, 2, 3]:\n",
    "        top100_overlap[exp][f\"{n}grams\"] = get_overlap_top_n_entities(\n",
    "            ngram_counter[exp][n], ngram_counter[\"Real\"][n], topn=100\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_syn_overlap = defaultdict(dict)\n",
    "_selected_experiments = sorted(\n",
    "    _selected_experiments\n",
    ")  # Make sure the list is sorted for consistency\n",
    "\n",
    "for i, exp1 in enumerate(_selected_experiments):\n",
    "    for j, exp2 in enumerate(_selected_experiments):\n",
    "        if j <= i:  # Only consider pairs in the upper triangle (including diagonal)\n",
    "            key = f\"{exp1} -> {exp2}\"\n",
    "            top100_syn_overlap[key][\"hashtags\"] = get_overlap_top_n_entities(\n",
    "                hashtag_counter[exp1], hashtag_counter[exp2], topn=100\n",
    "            )\n",
    "            top100_syn_overlap[key][\"usertags\"] = get_overlap_top_n_entities(\n",
    "                usertag_counter[exp1], usertag_counter[exp2], topn=100\n",
    "            )\n",
    "            for n in [1, 2, 3]:\n",
    "                top100_syn_overlap[key][f\"{n}grams\"] = get_overlap_top_n_entities(\n",
    "                    ngram_counter[exp1][n], ngram_counter[exp2][n], topn=100\n",
    "                )\n",
    "\n",
    "pd.DataFrame(top100_syn_overlap).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in selected_experiments:\n",
    "    print(f)\n",
    "    print(get_top_n_entities(hashtag_counter[f], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_not_in_real = defaultdict(dict)\n",
    "\n",
    "for exp in _selected_experiments:\n",
    "    top100_not_in_real[exp][\"hashtags\"] = get_top_n_entities(\n",
    "        hashtag_counter[exp], topn=100\n",
    "    ).difference(get_top_n_entities(hashtag_counter[\"Real\"], topn=100))\n",
    "    top100_not_in_real[exp][\"usertags\"] = get_top_n_entities(\n",
    "        usertag_counter[exp], topn=100\n",
    "    ).difference(get_top_n_entities(usertag_counter[\"Real\"], topn=100))\n",
    "    for n in [1, 2, 3]:\n",
    "        top100_not_in_real[exp][f\"{n}grams\"] = get_top_n_entities(\n",
    "            ngram_counter[exp][n], topn=100\n",
    "        ).difference(get_top_n_entities(ngram_counter[\"Real\"][n], topn=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_not_in_synthetic = defaultdict(dict)\n",
    "\n",
    "for exp in _selected_experiments:\n",
    "    top100_not_in_synthetic[exp][\"hashtags\"] = get_top_n_entities(\n",
    "        hashtag_counter[\"Real\"], topn=100\n",
    "    ).difference(get_top_n_entities(hashtag_counter[exp], topn=100))\n",
    "    top100_not_in_synthetic[exp][\"usertags\"] = get_top_n_entities(\n",
    "        usertag_counter[\"Real\"], topn=100\n",
    "    ).difference(get_top_n_entities(usertag_counter[exp], topn=100))\n",
    "    for n in [1, 2, 3]:\n",
    "        top100_not_in_synthetic[exp][f\"{n}grams\"] = get_top_n_entities(\n",
    "            ngram_counter[\"Real\"][n], topn=100\n",
    "        ).difference(get_top_n_entities(ngram_counter[exp][n], topn=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in _selected_experiments:\n",
    "    print(f)\n",
    "    print(list(top100_not_in_real[f][\"3grams\"])[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in _selected_experiments:\n",
    "    print(f)\n",
    "    print(list(top100_not_in_synthetic[f][\"3grams\"])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in _selected_experiments:\n",
    "    print(f)\n",
    "    print(list(top100_not_in_real[f][\"hashtags\"])[:10])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
