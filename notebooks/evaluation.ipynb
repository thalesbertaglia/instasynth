{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Synthetic Data\n",
    "\n",
    "This notebook includes the code used to evaluate the experiments and generate the `agg_df` dataframe used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import iglob\n",
    "from instasynth import evaluation, embedding_generation\n",
    "import pandas as pd\n",
    "\n",
    "real_data = (\n",
    "    pd.read_pickle(\"../data/df_sample.pkl\")\n",
    "    .sample(1000, random_state=42)\n",
    "    .dropna()\n",
    "    .query(\"caption ! = ''\")\n",
    ")\n",
    "emb_storage = embedding_generation.EmbeddingStorage(\n",
    "    Path(\"../embeddings/\"), embedding_file_name=\"embeddings.pkl\"\n",
    ")\n",
    "experiment_paths = [Path(f) for f in iglob(\"../results/*\") if \"archive\" not in f]\n",
    "\n",
    "test_spons_data = pd.read_pickle(\"../data/kim_sample_mini.pkl\")\n",
    "ann_data = pd.read_pickle(\"../data/ann_sample_ad_detection.pkl\")\n",
    "\n",
    "\n",
    "evaluator = evaluation.ExperimentEvaluator(\n",
    "    experiment_paths=experiment_paths,\n",
    "    test_dataset_ads=test_spons_data,\n",
    "    test_dataset_ads_undisclosed=ann_data,\n",
    "    real_dataset=real_data.copy(),\n",
    "    embedding_storage=emb_storage,\n",
    ")\n",
    "\n",
    "evaluator.load_experiment_metrics()\n",
    "evaluator.load_real_dataset_metrics()\n",
    "\n",
    "aggregated_df = evaluator.aggregate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the averaged metrics from the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_bootstrap_real = (\n",
    "    pd.read_pickle(\"../data/metrics_bootstrap_real.pkl\").loc[\"Real\"].to_dict()\n",
    ")\n",
    "\n",
    "for col in aggregated_df.columns:\n",
    "    aggregated_df.at[\"Real\", col] = metrics_bootstrap_real.get(\n",
    "        col, aggregated_df.at[\"Real\", col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the metrics diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator._real_dataset_metrics = aggregated_df.loc[\"Real\"].to_dict()\n",
    "difference_df = evaluator.compare_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding missing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_cosine_sim_diff = pd.DataFrame(\n",
    "    aggregated_df[\"real_internal_cosine_sim\"]\n",
    "    - aggregated_df[\"synthetic_internal_cosine_sim\"],\n",
    "    columns=[\"internal_cosine_sim\"],\n",
    ").T\n",
    "diff_df = pd.concat([difference_df.copy(), internal_cosine_sim_diff], axis=0)\n",
    "diff_df.drop(columns=[\"Real\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ignore = [\n",
    "    c\n",
    "    for c in aggregated_df.columns\n",
    "    if \"request\" in c\n",
    "    or \"_rate\" in c\n",
    "    or \"_internal_cosine_sim\" in c\n",
    "    or \"number_of_errors\" in c\n",
    "]\n",
    "columns_clf = [\n",
    "    c\n",
    "    for c in aggregated_df.columns\n",
    "    if \"ad_detection\" in c or \"pct_unique_captions\" in c\n",
    "]\n",
    "columns_na = diff_df.T.columns[diff_df.T.isna().all()].tolist()\n",
    "columns_compare = list(set(columns_na) - set(columns_ignore))\n",
    "columns_argmax = columns_clf + columns_compare\n",
    "columns_argmin = [\n",
    "    c for c in diff_df.T.columns if c not in columns_argmax and c not in columns_ignore\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.to_pickle(\"diff_df.pkl\")\n",
    "aggregated_df.to_pickle(\"aggregated_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
